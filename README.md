# Machine Learning From Scratch ğŸ§ 

A comprehensive, educational repository containing **machine learning algorithms implemented entirely from scratch** using **pure NumPy**, with a strong focus on **understanding the mathematics, optimization, and design principles** behind each model.

This repository is designed to help learners move beyond black-box libraries and truly understand **how machine learning works internally**.

---

## ğŸ¯ Goals of This Repository

- Implement ML algorithms **without using scikit-learn, PyTorch, or TensorFlow**
- Build strong intuition for **loss functions, gradients, and optimization**
- Provide **clean, readable, and well-documented implementations**
- Serve as a **learning reference**, not just a code dump
- Help with **interview preparation and concept revision**

---
```
## ğŸ“‚ Repository Structure

machine-learning-from-scratch/
â”‚
â”œâ”€â”€ regression/
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ linear_regression/
â”‚ â”œâ”€â”€ multiple_linear_regression/
â”‚ â””â”€â”€ huber_regression/
â”‚
â”œâ”€â”€ classification/
â”‚ â”œâ”€â”€ README.md
â”‚ â”œâ”€â”€ logistic_regression/
â”‚ â””â”€â”€ svm/
â”‚
â”œâ”€â”€ core/ # (optional / future)
â”‚ â”œâ”€â”€ base_model.py
â”‚ â”œâ”€â”€ metrics.py
â”‚ â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ datasets/ # (toy datasets only)
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


Each model follows a consistent structure:
model_name/
â”œâ”€â”€ model.py # Core implementation
â”œâ”€â”€ demo.py # Example usage
â””â”€â”€ notes.md # Theory + math explanation
```

---

## ğŸ“Œ Implemented Algorithms

### ğŸ”¹ Regression
- Linear Regression
- Multiple Linear Regression
- Huber Regression (Robust Regression)

### ğŸ”¹ Classification
- Logistic Regression
- Support Vector Machine (Linear SVM)

---

## ğŸ§  What You Will Learn

By studying this repository, you will understand:

- How loss functions are derived and optimized
- How gradient descent works internally
- Why normalization is important
- Differences between regression and classification
- How margin-based classifiers (SVM) work
- How robustness to outliers is achieved (Huber Loss)

---

## ğŸ§ª Design Philosophy

- âœ… Pure NumPy implementations
- âœ… Explicit math and gradients
- âœ… No hidden abstractions
- âœ… Clarity over micro-optimizations
- âŒ No black-box ML libraries

---

## ğŸš€ Recommended Learning Path

1. Linear Regression  
2. Multiple Linear Regression  
3. Huber Regression  
4. Logistic Regression  
5. Support Vector Machine  

This progression mirrors how ML concepts build on each other.

---

## ğŸ”® Planned Additions

- Perceptron
- Softmax Regression (Multiclass Logistic)
- Kernel SVM
- Naive Bayes
- Decision Trees (from scratch)
- KNN
- Ensemble methods (Bagging / Boosting)
- Basic Neural Networks (NumPy)

---

## ğŸ“š References & Inspiration

- Andrew Ng â€“ *Machine Learning*
- Trevor Hastie et al. â€“ *The Elements of Statistical Learning*
- Vapnik â€“ *Statistical Learning Theory*

---

## â­ Final Note

This repository prioritizes **learning and correctness** over performance.
If you find it useful, consider â­ starring the repo.

Contributions, suggestions, and improvements are welcome.